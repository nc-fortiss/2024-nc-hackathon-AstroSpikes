{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Global Akida workflow\n",
    "\n",
    ".. Note:: Please refer to the TensorFlow  [tf.keras.models](https://www.tensorflow.org/api_docs/python/tf/keras/models)_\n",
    "          module for model creation/import details and the [TensorFlow Guide](https://www.tensorflow.org/guide)_ for TensorFlow usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create and train\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:17:00.144045: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-12 16:17:00.532630: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-12 16:17:00.532799: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-12 16:17:00.600123: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-12 16:17:00.743511: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-12 16:17:00.745345: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-12 16:17:02.773004: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load and reshape projection dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#TODO: import the dataset\n",
    "\n",
    "#TODO: Load projection dataset\n",
    "# x_test, y_test = ...\n",
    "\n",
    "# Display a few images from the test set\n",
    "f, axarr = plt.subplots(1, 4)\n",
    "for i in range(0, 4):\n",
    "    pass\n",
    "    #TODO: adjust the following line to display the images\n",
    "    # axarr[i].imshow(x_test[i].reshape((28, 28)), cmap=cm.Greys_r)\n",
    "    # axarr[i].set_title('Class %d' % y_test[i])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Model definition\n",
    "\n",
    "Note that at this stage, there is nothing specific to the Akida IP.\n",
    "The model constructed below, as inspired by [this example](https://www.tensorflow.org/model_optimization/guide/quantization/training_example#train_a_model_for_mnist_without_quantization_aware_training)_,\n",
    "is a completely standard [Keras](https://www.tensorflow.org/api_docs/python/tf/keras)_ CNN model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jskupien/.local/lib/python3.10/site-packages/keras/src/applications/mobilenet.py:207: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 2 input channels.\n",
      "  input_shape = imagenet_utils.obtain_input_shape(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 240, 240, 2)]     0         \n",
      "                                                                 \n",
      " conv1 (Conv2D)              (None, 120, 120, 32)      576       \n",
      "                                                                 \n",
      " conv1_bn (BatchNormalizati  (None, 120, 120, 32)      128       \n",
      " on)                                                             \n",
      "                                                                 \n",
      " conv1_relu (ReLU)           (None, 120, 120, 32)      0         \n",
      "                                                                 \n",
      " conv_dw_1 (DepthwiseConv2D  (None, 120, 120, 32)      288       \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_1_bn (BatchNormali  (None, 120, 120, 32)      128       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_1_relu (ReLU)       (None, 120, 120, 32)      0         \n",
      "                                                                 \n",
      " conv_pw_1 (Conv2D)          (None, 120, 120, 64)      2048      \n",
      "                                                                 \n",
      " conv_pw_1_bn (BatchNormali  (None, 120, 120, 64)      256       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_1_relu (ReLU)       (None, 120, 120, 64)      0         \n",
      "                                                                 \n",
      " conv_pad_2 (ZeroPadding2D)  (None, 121, 121, 64)      0         \n",
      "                                                                 \n",
      " conv_dw_2 (DepthwiseConv2D  (None, 60, 60, 64)        576       \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_2_bn (BatchNormali  (None, 60, 60, 64)        256       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_2_relu (ReLU)       (None, 60, 60, 64)        0         \n",
      "                                                                 \n",
      " conv_pw_2 (Conv2D)          (None, 60, 60, 128)       8192      \n",
      "                                                                 \n",
      " conv_pw_2_bn (BatchNormali  (None, 60, 60, 128)       512       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_2_relu (ReLU)       (None, 60, 60, 128)       0         \n",
      "                                                                 \n",
      " conv_dw_3 (DepthwiseConv2D  (None, 60, 60, 128)       1152      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_3_bn (BatchNormali  (None, 60, 60, 128)       512       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_3_relu (ReLU)       (None, 60, 60, 128)       0         \n",
      "                                                                 \n",
      " conv_pw_3 (Conv2D)          (None, 60, 60, 128)       16384     \n",
      "                                                                 \n",
      " conv_pw_3_bn (BatchNormali  (None, 60, 60, 128)       512       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_3_relu (ReLU)       (None, 60, 60, 128)       0         \n",
      "                                                                 \n",
      " conv_pad_4 (ZeroPadding2D)  (None, 61, 61, 128)       0         \n",
      "                                                                 \n",
      " conv_dw_4 (DepthwiseConv2D  (None, 30, 30, 128)       1152      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_4_bn (BatchNormali  (None, 30, 30, 128)       512       \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_4_relu (ReLU)       (None, 30, 30, 128)       0         \n",
      "                                                                 \n",
      " conv_pw_4 (Conv2D)          (None, 30, 30, 256)       32768     \n",
      "                                                                 \n",
      " conv_pw_4_bn (BatchNormali  (None, 30, 30, 256)       1024      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_4_relu (ReLU)       (None, 30, 30, 256)       0         \n",
      "                                                                 \n",
      " conv_dw_5 (DepthwiseConv2D  (None, 30, 30, 256)       2304      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_5_bn (BatchNormali  (None, 30, 30, 256)       1024      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_5_relu (ReLU)       (None, 30, 30, 256)       0         \n",
      "                                                                 \n",
      " conv_pw_5 (Conv2D)          (None, 30, 30, 256)       65536     \n",
      "                                                                 \n",
      " conv_pw_5_bn (BatchNormali  (None, 30, 30, 256)       1024      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_5_relu (ReLU)       (None, 30, 30, 256)       0         \n",
      "                                                                 \n",
      " conv_pad_6 (ZeroPadding2D)  (None, 31, 31, 256)       0         \n",
      "                                                                 \n",
      " conv_dw_6 (DepthwiseConv2D  (None, 15, 15, 256)       2304      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_6_bn (BatchNormali  (None, 15, 15, 256)       1024      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_6_relu (ReLU)       (None, 15, 15, 256)       0         \n",
      "                                                                 \n",
      " conv_pw_6 (Conv2D)          (None, 15, 15, 512)       131072    \n",
      "                                                                 \n",
      " conv_pw_6_bn (BatchNormali  (None, 15, 15, 512)       2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_6_relu (ReLU)       (None, 15, 15, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_7 (DepthwiseConv2D  (None, 15, 15, 512)       4608      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_7_bn (BatchNormali  (None, 15, 15, 512)       2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_7_relu (ReLU)       (None, 15, 15, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_7 (Conv2D)          (None, 15, 15, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_7_bn (BatchNormali  (None, 15, 15, 512)       2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_7_relu (ReLU)       (None, 15, 15, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_8 (DepthwiseConv2D  (None, 15, 15, 512)       4608      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_8_bn (BatchNormali  (None, 15, 15, 512)       2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_8_relu (ReLU)       (None, 15, 15, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_8 (Conv2D)          (None, 15, 15, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_8_bn (BatchNormali  (None, 15, 15, 512)       2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_8_relu (ReLU)       (None, 15, 15, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_9 (DepthwiseConv2D  (None, 15, 15, 512)       4608      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_9_bn (BatchNormali  (None, 15, 15, 512)       2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_dw_9_relu (ReLU)       (None, 15, 15, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_9 (Conv2D)          (None, 15, 15, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_9_bn (BatchNormali  (None, 15, 15, 512)       2048      \n",
      " zation)                                                         \n",
      "                                                                 \n",
      " conv_pw_9_relu (ReLU)       (None, 15, 15, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_10 (DepthwiseConv2  (None, 15, 15, 512)       4608      \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv_dw_10_bn (BatchNormal  (None, 15, 15, 512)       2048      \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_dw_10_relu (ReLU)      (None, 15, 15, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_10 (Conv2D)         (None, 15, 15, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_10_bn (BatchNormal  (None, 15, 15, 512)       2048      \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_pw_10_relu (ReLU)      (None, 15, 15, 512)       0         \n",
      "                                                                 \n",
      " conv_dw_11 (DepthwiseConv2  (None, 15, 15, 512)       4608      \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv_dw_11_bn (BatchNormal  (None, 15, 15, 512)       2048      \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_dw_11_relu (ReLU)      (None, 15, 15, 512)       0         \n",
      "                                                                 \n",
      " conv_pw_11 (Conv2D)         (None, 15, 15, 512)       262144    \n",
      "                                                                 \n",
      " conv_pw_11_bn (BatchNormal  (None, 15, 15, 512)       2048      \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_pw_11_relu (ReLU)      (None, 15, 15, 512)       0         \n",
      "                                                                 \n",
      " conv_pad_12 (ZeroPadding2D  (None, 16, 16, 512)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_12 (DepthwiseConv2  (None, 7, 7, 512)         4608      \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv_dw_12_bn (BatchNormal  (None, 7, 7, 512)         2048      \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_dw_12_relu (ReLU)      (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " conv_pw_12 (Conv2D)         (None, 7, 7, 1024)        524288    \n",
      "                                                                 \n",
      " conv_pw_12_bn (BatchNormal  (None, 7, 7, 1024)        4096      \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_pw_12_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
      "                                                                 \n",
      " conv_dw_13 (DepthwiseConv2  (None, 7, 7, 1024)        9216      \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv_dw_13_bn (BatchNormal  (None, 7, 7, 1024)        4096      \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_dw_13_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
      "                                                                 \n",
      " conv_pw_13 (Conv2D)         (None, 7, 7, 1024)        1048576   \n",
      "                                                                 \n",
      " conv_pw_13_bn (BatchNormal  (None, 7, 7, 1024)        4096      \n",
      " ization)                                                        \n",
      "                                                                 \n",
      " conv_pw_13_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 50176)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 7)                 351239    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3579815 (13.66 MB)\n",
      "Trainable params: 3557927 (13.57 MB)\n",
      "Non-trainable params: 21888 (85.50 KB)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "### DEFINE MODEL\n",
    "base_model = keras.applications.MobileNet(include_top=False, input_shape=(240, 240, 2), weights=None)\n",
    "# base_model.trainable = False  # Freeze all layers in the base model\n",
    "\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "output_l = tf.keras.layers.Dense(7, activation='linear')(x)\n",
    "model_keras = Model(inputs=base_model.input, outputs=output_l)\n",
    "\n",
    "print(model_keras.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Model training\n",
    "\n",
    "Given the model created above, train the model and check its accuracy. The model should achieve\n",
    "a test accuracy over 98% after 10 epochs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_keras.compile(\n",
    "    loss=None, #TODO: define the loss function\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "_ = model_keras.fit(x_train, y_train, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = model_keras.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quantize\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. 8-bit quantization\n",
    "\n",
    "An Akida accelerator processes 8 or 4-bits integer activations and weights. Therefore,\n",
    "the floating point Keras model must be quantized in preparation to run on an Akida accelerator.\n",
    "\n",
    "The QuantizeML [quantize](../../api_reference/quantizeml_apis.html#quantizeml.models.quantize)_\n",
    "function can be used to quantize a Keras model for Akida. For this step in this example, an\n",
    "“8/8/8” quantization scheme will be applied to the floating point Keras model to produce 8-bit\n",
    "weights in the first layer, 8-bit weights in all other layers, and 8-bit activations.\n",
    "\n",
    "The quantization process results in a Keras model with custom [QuantizeML quantized layers](../../api_reference/quantizeml_apis.html#layers)_ substituted for the original Keras layers.\n",
    "All Keras API functions can be applied on this new model: ``summary()``, ``compile()``, ``fit()``. etc.\n",
    "\n",
    ".. Note:: The ``quantize`` function applies [several transformations](../../api_reference/quantizeml_apis.html#transforms)_ to\n",
    "          the original model. For example, it folds the batch normalization layers into the\n",
    "          corresponding neural layers. The new weights are computed according to this folding\n",
    "          operation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 16:31:27.591525: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 117964800 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024/1024 [==============================] - 26s 24ms/step\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 240, 240, 2)]     0         \n",
      "                                                                 \n",
      " conv1 (QuantizedConv2D)     (None, 120, 120, 32)      608       \n",
      "                                                                 \n",
      " conv1_relu (QuantizedReLU)  (None, 120, 120, 32)      64        \n",
      "                                                                 \n",
      " conv_dw_1 (QuantizedDepthw  (None, 120, 120, 32)      320       \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " conv_dw_1_relu (QuantizedR  (None, 120, 120, 32)      64        \n",
      " eLU)                                                            \n",
      "                                                                 \n",
      " conv_pw_1 (QuantizedConv2D  (None, 120, 120, 64)      2112      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_pw_1_relu (QuantizedR  (None, 120, 120, 64)      128       \n",
      " eLU)                                                            \n",
      "                                                                 \n",
      " conv_dw_2 (QuantizedDepthw  (None, 60, 60, 64)        640       \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " conv_dw_2_relu (QuantizedR  (None, 60, 60, 64)        128       \n",
      " eLU)                                                            \n",
      "                                                                 \n",
      " conv_pw_2 (QuantizedConv2D  (None, 60, 60, 128)       8320      \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_pw_2_relu (QuantizedR  (None, 60, 60, 128)       256       \n",
      " eLU)                                                            \n",
      "                                                                 \n",
      " conv_dw_3 (QuantizedDepthw  (None, 60, 60, 128)       1280      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " conv_dw_3_relu (QuantizedR  (None, 60, 60, 128)       256       \n",
      " eLU)                                                            \n",
      "                                                                 \n",
      " conv_pw_3 (QuantizedConv2D  (None, 60, 60, 128)       16512     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_pw_3_relu (QuantizedR  (None, 60, 60, 128)       256       \n",
      " eLU)                                                            \n",
      "                                                                 \n",
      " conv_dw_4 (QuantizedDepthw  (None, 30, 30, 128)       1280      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " conv_dw_4_relu (QuantizedR  (None, 30, 30, 128)       256       \n",
      " eLU)                                                            \n",
      "                                                                 \n",
      " conv_pw_4 (QuantizedConv2D  (None, 30, 30, 256)       33024     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_pw_4_relu (QuantizedR  (None, 30, 30, 256)       512       \n",
      " eLU)                                                            \n",
      "                                                                 \n",
      " conv_dw_5 (QuantizedDepthw  (None, 30, 30, 256)       2560      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " conv_dw_5_relu (QuantizedR  (None, 30, 30, 256)       512       \n",
      " eLU)                                                            \n",
      "                                                                 \n",
      " conv_pw_5 (QuantizedConv2D  (None, 30, 30, 256)       65792     \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_pw_5_relu (QuantizedR  (None, 30, 30, 256)       512       \n",
      " eLU)                                                            \n",
      "                                                                 \n",
      " conv_dw_6 (QuantizedDepthw  (None, 15, 15, 256)       2560      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " conv_dw_6_relu (QuantizedR  (None, 15, 15, 256)       512       \n",
      " eLU)                                                            \n",
      "                                                                 \n",
      " conv_pw_6 (QuantizedConv2D  (None, 15, 15, 512)       131584    \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_pw_6_relu (QuantizedR  (None, 15, 15, 512)       1024      \n",
      " eLU)                                                            \n",
      "                                                                 \n",
      " conv_dw_7 (QuantizedDepthw  (None, 15, 15, 512)       5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " conv_dw_7_relu (QuantizedR  (None, 15, 15, 512)       1024      \n",
      " eLU)                                                            \n",
      "                                                                 \n",
      " conv_pw_7 (QuantizedConv2D  (None, 15, 15, 512)       262656    \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_pw_7_relu (QuantizedR  (None, 15, 15, 512)       1024      \n",
      " eLU)                                                            \n",
      "                                                                 \n",
      " conv_dw_8 (QuantizedDepthw  (None, 15, 15, 512)       5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " conv_dw_8_relu (QuantizedR  (None, 15, 15, 512)       1024      \n",
      " eLU)                                                            \n",
      "                                                                 \n",
      " conv_pw_8 (QuantizedConv2D  (None, 15, 15, 512)       262656    \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_pw_8_relu (QuantizedR  (None, 15, 15, 512)       1024      \n",
      " eLU)                                                            \n",
      "                                                                 \n",
      " conv_dw_9 (QuantizedDepthw  (None, 15, 15, 512)       5120      \n",
      " iseConv2D)                                                      \n",
      "                                                                 \n",
      " conv_dw_9_relu (QuantizedR  (None, 15, 15, 512)       1024      \n",
      " eLU)                                                            \n",
      "                                                                 \n",
      " conv_pw_9 (QuantizedConv2D  (None, 15, 15, 512)       262656    \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_pw_9_relu (QuantizedR  (None, 15, 15, 512)       1024      \n",
      " eLU)                                                            \n",
      "                                                                 \n",
      " conv_dw_10 (QuantizedDepth  (None, 15, 15, 512)       5120      \n",
      " wiseConv2D)                                                     \n",
      "                                                                 \n",
      " conv_dw_10_relu (Quantized  (None, 15, 15, 512)       1024      \n",
      " ReLU)                                                           \n",
      "                                                                 \n",
      " conv_pw_10 (QuantizedConv2  (None, 15, 15, 512)       262656    \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv_pw_10_relu (Quantized  (None, 15, 15, 512)       1024      \n",
      " ReLU)                                                           \n",
      "                                                                 \n",
      " conv_dw_11 (QuantizedDepth  (None, 15, 15, 512)       5120      \n",
      " wiseConv2D)                                                     \n",
      "                                                                 \n",
      " conv_dw_11_relu (Quantized  (None, 15, 15, 512)       1024      \n",
      " ReLU)                                                           \n",
      "                                                                 \n",
      " conv_pw_11 (QuantizedConv2  (None, 15, 15, 512)       262656    \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv_pw_11_relu (Quantized  (None, 15, 15, 512)       1024      \n",
      " ReLU)                                                           \n",
      "                                                                 \n",
      " conv_pw_11_relu/dequantize  (None, 15, 15, 512)       0         \n",
      " r (Dequantizer)                                                 \n",
      "                                                                 \n",
      " conv_pad_12 (ZeroPadding2D  (None, 16, 16, 512)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv_dw_12 (DepthwiseConv2  (None, 7, 7, 512)         5120      \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv_dw_12_relu (ReLU)      (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " conv_pw_12 (Conv2D)         (None, 7, 7, 1024)        525312    \n",
      "                                                                 \n",
      " conv_pw_12_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
      "                                                                 \n",
      " conv_dw_13 (DepthwiseConv2  (None, 7, 7, 1024)        10240     \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv_dw_13_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
      "                                                                 \n",
      " conv_pw_13 (Conv2D)         (None, 7, 7, 1024)        1049600   \n",
      "                                                                 \n",
      " conv_pw_13_relu (ReLU)      (None, 7, 7, 1024)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 50176)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 7)                 351239    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3561703 (13.59 MB)\n",
      "Trainable params: 3546983 (13.53 MB)\n",
      "Non-trainable params: 14720 (57.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from quantizeml.models import quantize, QuantizationParams\n",
    "\n",
    "qparams = QuantizationParams(input_weight_bits=8, weight_bits=8, activation_bits=8)\n",
    "model_quantized = quantize(model_keras, qparams=qparams)\n",
    "model_quantized.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. Note:: Note that the number of parameters for the floating and quantized models differs,\n",
    "          a consequence of the BatchNormalization folding and the additional parameters\n",
    "          added for quantization. For further details, please refer to their respective summary.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the quantized model accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compile_evaluate(model):\n",
    "    \"\"\" Compiles and evaluates the model, then return accuracy score. \"\"\"\n",
    "    model.compile(metrics=['accuracy'])\n",
    "    return model.evaluate(x_test, y_test, verbose=0)[1]\n",
    "\n",
    "\n",
    "print('Test accuracy after 8-bit quantization:', compile_evaluate(model_quantized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Effect of calibration\n",
    "\n",
    "The previous call to ``quantize`` was made with random samples for calibration\n",
    "(default parameters). While the observed drop in accuracy is minimal, that is\n",
    "around 1%, it can be worse on more complex models. Therefore, it is advised to\n",
    "use a set of real samples from the training set for calibration during a call\n",
    "to ``quantize``.\n",
    "Note that this remains a calibration step rather than a training step in that\n",
    "no output labels are required. Furthermore, any relevant data could be used for\n",
    "calibration. The recommended settings for calibration that are widely used to\n",
    "obtain the [zoo performance](../../model_zoo_performance.html#akida-2-0-models)_ are:\n",
    "\n",
    "- 1024 samples\n",
    "- a batch size of 100\n",
    "- 2 epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_quantized = quantize(model_keras, qparams=qparams,\n",
    "                           samples=x_train, num_samples=1024, batch_size=100, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the accuracy for the quantized and calibrated model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Test accuracy after calibration:', compile_evaluate(model_quantized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibrating with real samples on this model recovers the initial float accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. 4-bit quantization\n",
    "\n",
    "The accuracy of the 8/8/8 quantized model is equal to that of the Keras floating point\n",
    "model. In some cases, a smaller memory size for the model is required. This can be\n",
    "accomplished through quantization of the model to smaller bitwidths.\n",
    "\n",
    "The model will now be quantized to 8/4/4, that is 8-bit weights in the first layer with\n",
    "4-bit weights and activations in all other layers. Such a quantization scheme will usually\n",
    "introduce a performance drop.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qparams = QuantizationParams(input_weight_bits=8, weight_bits=4, activation_bits=4)\n",
    "model_quantized = quantize(model_keras, qparams=qparams,\n",
    "                           samples=x_train, num_samples=1024, batch_size=100, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the 4-bit quantized accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Test accuracy after 4-bit quantization:', compile_evaluate(model_quantized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Model fine tuning (Quantization Aware Training)\n",
    "\n",
    "When a model suffers from an accuracy drop after quantization, fine tuning or Quantization\n",
    "Aware Training (QAT) may recover some or all of the original performance.\n",
    "\n",
    "Note that since this is a fine tuning step, both the number of epochs and learning rate are\n",
    "expected to be lower than during the initial float training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_quantized.compile(\n",
    "    loss=None, #TODO: define the loss function\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "model_quantized.fit(x_train, y_train, epochs=5, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = model_quantized.evaluate(x_test, y_test, verbose=0)[1]\n",
    "print('Test accuracy after fine tuning:', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Convert\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Convert to Akida model\n",
    "\n",
    "When the quantized model produces satisfactory performance, it can be converted to the native\n",
    "Akida format. The [convert](../../api_reference/cnn2snn_apis.html#cnn2snn.convert)_ function\n",
    "returns a model in Akida format ready for inference.\n",
    "\n",
    "As with Keras, the summary() method provides a textual representation of the Akida model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function NonTrackVariable.set_var at 0x756544728b80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function NonTrackVariable.set_var at 0x7565447293f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jskupien/.local/lib/python3.10/site-packages/cnn2snn/quantizeml/blocks.py:160: UserWarning: Conversion stops at layer conv_pw_11_relu because of a dequantizer. The end of the model is ignored:\n",
      "___________________________________________________\n",
      "Layer (type)\n",
      "===================================================\n",
      "conv_pad_12 (ZeroPadding2D)\n",
      "conv_dw_12 (DepthwiseConv2D)\n",
      "conv_dw_12_relu (ReLU)\n",
      "conv_pw_12 (Conv2D)\n",
      "conv_pw_12_relu (ReLU)\n",
      "conv_dw_13 (DepthwiseConv2D)\n",
      "conv_dw_13_relu (ReLU)\n",
      "conv_pw_13 (Conv2D)\n",
      "conv_pw_13_relu (ReLU)\n",
      "flatten (Flatten)\n",
      "dense (Dense)\n",
      "===================================================\n",
      "\n",
      "  warnings.warn(\"Conversion stops\" + stop_layer_msg + \" because of a dequantizer. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Model Summary                  \n",
      "_________________________________________________\n",
      "Input shape    Output shape   Sequences  Layers\n",
      "=================================================\n",
      "[240, 240, 2]  [15, 15, 512]  1          25    \n",
      "_________________________________________________\n",
      "\n",
      "_____________________________________________________________________________\n",
      "Layer (type)                               Output shape    Kernel shape    \n",
      "\n",
      "============== SW/conv1-conv_pw_11_relu/dequantizer (Software) ==============\n",
      "\n",
      "conv1 (Conv2D)                             [120, 120, 32]  (3, 3, 2, 32)   \n",
      "_____________________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)                [120, 120, 32]  (3, 3, 32, 1)   \n",
      "_____________________________________________________________________________\n",
      "conv_pw_1 (Conv2D)                         [120, 120, 64]  (1, 1, 32, 64)  \n",
      "_____________________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)                [60, 60, 64]    (3, 3, 64, 1)   \n",
      "_____________________________________________________________________________\n",
      "conv_pw_2 (Conv2D)                         [60, 60, 128]   (1, 1, 64, 128) \n",
      "_____________________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)                [60, 60, 128]   (3, 3, 128, 1)  \n",
      "_____________________________________________________________________________\n",
      "conv_pw_3 (Conv2D)                         [60, 60, 128]   (1, 1, 128, 128)\n",
      "_____________________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)                [30, 30, 128]   (3, 3, 128, 1)  \n",
      "_____________________________________________________________________________\n",
      "conv_pw_4 (Conv2D)                         [30, 30, 256]   (1, 1, 128, 256)\n",
      "_____________________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)                [30, 30, 256]   (3, 3, 256, 1)  \n",
      "_____________________________________________________________________________\n",
      "conv_pw_5 (Conv2D)                         [30, 30, 256]   (1, 1, 256, 256)\n",
      "_____________________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)                [15, 15, 256]   (3, 3, 256, 1)  \n",
      "_____________________________________________________________________________\n",
      "conv_pw_6 (Conv2D)                         [15, 15, 512]   (1, 1, 256, 512)\n",
      "_____________________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)                [15, 15, 512]   (3, 3, 512, 1)  \n",
      "_____________________________________________________________________________\n",
      "conv_pw_7 (Conv2D)                         [15, 15, 512]   (1, 1, 512, 512)\n",
      "_____________________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)                [15, 15, 512]   (3, 3, 512, 1)  \n",
      "_____________________________________________________________________________\n",
      "conv_pw_8 (Conv2D)                         [15, 15, 512]   (1, 1, 512, 512)\n",
      "_____________________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)                [15, 15, 512]   (3, 3, 512, 1)  \n",
      "_____________________________________________________________________________\n",
      "conv_pw_9 (Conv2D)                         [15, 15, 512]   (1, 1, 512, 512)\n",
      "_____________________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D)               [15, 15, 512]   (3, 3, 512, 1)  \n",
      "_____________________________________________________________________________\n",
      "conv_pw_10 (Conv2D)                        [15, 15, 512]   (1, 1, 512, 512)\n",
      "_____________________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D)               [15, 15, 512]   (3, 3, 512, 1)  \n",
      "_____________________________________________________________________________\n",
      "conv_pw_11 (Conv2D)                        [15, 15, 512]   (1, 1, 512, 512)\n",
      "_____________________________________________________________________________\n",
      "conv_pw_11_relu/dequantizer (Dequantizer)  [15, 15, 512]   N/A             \n",
      "_____________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from cnn2snn import convert\n",
    "\n",
    "model_akida = convert(model_quantized)\n",
    "model_akida.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Check performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy = model_akida.evaluate(x_test, y_test)\n",
    "print('Test accuracy after conversion:', accuracy)\n",
    "\n",
    "# For non-regression purposes\n",
    "assert accuracy > 0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Show predictions for a single image\n",
    "\n",
    "Display one of the test images, such as the first image in the dataset from above, to visualize\n",
    "the output of the model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test a single example\n",
    "sample_image = 0\n",
    "image = x_test[sample_image]\n",
    "outputs = model_akida.predict(image.reshape(1, 28, 28, 1))\n",
    "print('Input Label: %i' % y_test[sample_image])\n",
    "\n",
    "f, axarr = plt.subplots(1, 2)\n",
    "#TODO: adjust the following line to display the image\n",
    "# axarr[0].imshow(x_test[sample_image].reshape((28, 28)), cmap=cm.Greys_r)\n",
    "axarr[0].set_title('Class %d' % y_test[sample_image])\n",
    "axarr[1].bar(range(10), outputs.squeeze())\n",
    "axarr[1].set_xticks(range(10))\n",
    "plt.show()\n",
    "\n",
    "print(outputs.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the output from the model above. As is typical in backprop-trained models, the final\n",
    "layer is a Dense layer with one neuron for each of the 10 classes in the dataset. The goal of\n",
    "training is to maximize the response of the neuron corresponding to the label of each training\n",
    "sample while minimizing the responses of the other neurons.\n",
    "\n",
    "In the bar chart above, you can see the outputs from all 10 neurons. It is easy to see that neuron\n",
    "7 responds much more strongly than the others. The first sample is indeed a number 7.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
